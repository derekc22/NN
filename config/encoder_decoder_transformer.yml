architecture:
  ff_architecture:
    activations:
    - GELU
    - GELU
    neuron_counts:
    - 2048
    - 512
  transformer_architecture:
    d_model: 512
    depth: 6
    model_type: encoder-decoder
    num_heads: 8
hyperparameters:
  ff_hyperparameters:
    dropout_rate: null
    lambda_L2: 0.001
    learn_rate: 0.001
    optimizer: adam
  transformer_hyperparameters:
    batch_size: null
    dropout_rate: null
    lambda_L2: null
    learn_rate: 0.001
    loss_func: CELoss
    optimizer: adam
    reduction: mean
log_id: logs/2025-07-13_19:20:47_encoder_decoder_transformer
specifications:
  context_window: 512
  sequence_length: 128
system:
  device: cpu
  ff_save_fpath: ./params/transformer/ff
  save_fpath: ./params/transformer
test:
  show_results: false
  test_dataset_size: null
train:
  epochs: 250
  show_loss_plot: true
  train_dataset_size: null
