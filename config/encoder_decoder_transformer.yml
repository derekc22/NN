architecture:
  ff_architecture:
    activation_fns:
    - reLU
    - reLU
    neuron_counts:
    - 2048
    - 64
  transformer_architecture:
    d_model: 64
    depth: 6
    model_type: encoder-decoder
    num_heads: 8
hyperparameters:
  ff_hyperparameters:
    dropout_rate: null
    lambda_L2: 0.001
    learn_rate: 0.01
    optimizer: adam
  transformer_hyperparameters:
    batch_size: null
    dropout_rate: null
    lambda_L2: null
    learn_rate: 0.01
    loss_func: CELoss
    optimizer: adam
    reduction: mean
log_id: logs/2025-07-13_11:52:12_encoder_decoder_transformer
specifications:
  context_window: 512
  sequence_length: 128
system:
  device: cpu
  save_fpath: ./params/transformer
test:
  show_results: false
  test_dataset_size: null
train:
  epochs: 250
  show_loss_plot: true
  train_dataset_size: null
