05/17/2025
    - change if statements in src/network.py to use switch statements

05/18/2025
    - add validation loss tracking to the training loop
    - (DONE: 05/18/2025) change mlp layer numbering to start from 01 instead of 02 (ie 'layer_01_weights_sigmoid' instead of 'layer_02_weights_sigmoid')

05/22/2025
    - test rnn_text.py in autoregressive mode
    - integrate and test gradient clipping in other models' backward() methods
    - register/wrap weights/biases in torch.nn.Parameter() so that Network._collect_parameters() can be replaced with a call to torch.nn.Module.parameters() when attempting to clip gradients

05/25/2025
    - (DONE: 05/25/2025) change 'self.is_conv_layer' in the cnn class to self.type == 'conv' or 'pool' to match the conventions of other architectures
    - double check error checking fetchRNNParametersFromFile() in utils/rnn_utils.py

05/26/2025
    - save model weights as pkl instead of pth
    - (DONE: 05/26/2025) investigate solutions such that self.generateState() can be called with the batch size as an argument

05/27/2025
    - implement and test a lstm_text.py model

06/18/2025
    - get rid of 'Layer' class and instead just implement each activation function as a pure function instead of as a method of the class

06/21/2025
    - rename 'curr_input' to just 'X' in the training loop for all models
    - (DONE: 06/23/2025) rename convolutional.py and CNNLayer to something like cnn_layer.py and CNNLayer, respectively since (obviously) not all layer in CNNs are convolutional. This will aid readability and clarity.
    - add custom implementations of batchNorm and layerNorm to src/functions.py