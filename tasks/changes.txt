 mlp-reshape (05/18/2025)
    - create a new directory structure for the project
    - reshape mlp weight matrices from (input_features x data_set_size) to (data_set_size x input_features)
    - completely refactor training scripts to fit new mlp weight matrix shape
    - add config yaml files for each model
    - create separate main.py scripts for each model that use the config files instead of argparse
    - write symmetric tensor data generation function for mlp training/testing (gen_matrix_stack())
    - abstracted inference result printing and calculation into separate function (print_classification_results())
    - initialize logger and todo functionalities
    - initialize empty files for rnn, llm, lstm, and transformer model 
    - add changes.txt
    - add requirements.txt
    - add .gitignore

rnn (05/21/2025)
   - fully working rnn model, trained on sine wave data
   - supports stateful and stateless training/inference
   - added tanh activation function
   - added gradient clipping

   - added sine wave data generation function (gen_sine_wave())
   -- generates batch of sine wave data with specified number of timesteps
   -- includes arguments for adding noise and varying phase to ensure diverse training data

   - fixed bug where self.device was a string ('cpu') instead of a torch.device object (<torch.device('cpu')>)
   - split utils/data.py into multiple, model-specific files for better organization

   - added bash script, run.sh, to run training and inference all with one command 
   -- to run the script, use the command: ./run.sh <file_name> from the root directory...
   -- where <file_name> is the name of the main file to run (NOT including 'main_' or the extension '.py') (e.g. rnn, mlp, sine_rnn)
   -- note that the associated config file must have the same name as the main file (e.g. sine_rnn.yaml for main_sine_rnn.py)
   -- example usage to run the sine_rnn model: ./run.sh sine_rnn
   -- where the yaml file would be named: sine_rnn.yaml

   - started working on text generation with rnn


rnn-text (05/22/2025)
   - added text generation with rnn
   -- chatGPT calls it a 'recurrent autoencoder (more precisely, an embedding‑level RNN autoencoder).'
   -- why an encoder and not a decoder? ChatGPT says its not a decoder because:
   -- 1) you never feed its own previous output back in
   -- 2) there is no distinct “decoding” phase that takes a compressed summary and unfolds it into a new sequence of predictions
   -- a decoder generates an output sequence, usually autoregressively, one element at a time, conditioning each step on (a) the latent encoding and (b) everything generated so far
   
   - added functions to:
   -- load and manage GloVe word embeddings 
   -- encode/decode between words and vector representations
   -- process text at sentence and paragraph levels
   -- generate training data from text with appropriate labels
   --- note that labels are generated by shifting the text by one character and padding the last character with the word 'end'

   - as stated above, this model is not autoregressive. It does not pass its own previous output back in as the input for the next time step
   - rather, it uses the entire input data sequence to predict the next character in the sequence
   - this work can be extended down the line by adding autoregressive capabilities to the model


rnn-auto-regressive (05/23/2025)
   - successfully (?) added autoregressive capabilities to rnn model
   -- logic seems to be correct, but the model does not train properly, no matter what
   -- tried to remedy this problem by adding a 'teacher forcing' mechanism (progressive, regressive, etc), but to no avail
   -- will pause on rnn development for now and move on to lstm

   - working on this auto-regressive model revealed a logical bug in the original rnn model:
   -- t, X = gen_sine_wave() is what is returned from the gen_sine_wave() function
   -- in the non autoregressive model, t is used as the input to the model, which may be incorrect for the use case of rnns
   -- in this case, the model essentially learns a mapping from t to X, which is not the intended use case (i believe)
   -- the intended use case of these sequential models is to take some input sequence and predict the next element in the sequence
   -- thus, since t is not even 'in the space' of X, it is not a valid input to the model. that is my thinking 
   -- perhaps an analogy is dimensional homogeneity in an physics. t is not dimensionally homogeneous with X as 't' is a time variable and 'X' is a spatial variable
   -- thus, attempting to predict the next X from a value of t may not be a valid use case for an rnn or other sequential model
   -- perhaps this is what chatGPT meant when it said my original rnn model was an 'autoencoder' (or at least something similar)
   -- i believe that, if i want to predict the next X, the input to the model should be a sequence of X values, not t values
   -- thus, in the autoregressive model, X is directly fed into the model

   - one last thing to note about my two implementations (for documentation purposes)
   -- In the non autoregressive model, the only thing passed from t -> t+1 -> t+2 -> ... -> t+n is the 'hidden state' of the model
   -- In the autoregressive model, not only is the hidden state passed from t -> t+1 -> t+2 -> ... -> t+n, but also the output at the current time step

   - added decaying sine wave data generation function (gen_decaying_sine_wave())
   -- non autoregressive rnn model performs worse (abysmal) on this data than on the regular sine wave data
   -- in fact, both models (autoregressive and non autoregressive) perform worse on this data than on the regular sine wave data
   -- furthermore, when the equation is changed from  y=exp(-t)*sin(t) to y=exp(t)*sin(t) (that is, a growing sine wave instead of a decaying sine wave), both models perform even even worse

   - added logging script (utils/logger.py) to log config files after training
   -- added this script to 'run.sh' such that it runs automatically after training

   - added save_fpath argument to all models such that weights can be saved to a loaded from more from more flexible locations


lstm (05/27/2025)
   - changed rnn architecture from having a separate output layer to having the output transformation responsibilities (ie why, bhy, and the output activation) be attached to the final hidden layer
   -- this is an intentional design choice (i think it makes sense to have the output transformation as part of the final hidden layer)
   -- i believe that this architecture makes sense for the lstm, which is why i changed the rnn architecture to match

   - improved utils/logger.py such that data/plot_training_results logs the training loss curves (in addition to the config file) via a 'log_id' that is automatically generated (via datetime) and added to the config file
   -- also added functionality to data/plot_regression_results.py that saves the first 5 regression result plots to the new log directory as well

   - changed "if self.is_conv_layer:" in the cnn class to "if self.type == 'convolutional':" to match the conventions of other architectures

   - fixed really dumb rnn bug whereby the state was being reset after each epoch, even if self.stateful=True
   -- this was caused by me placing the call to self.generate_state() INSIDE the 'forward()' call instead of outside of it
   -- thus, the state attribute (self.ht1) would be re-initialized back to zeros after each epoch instead of being carried over to the next epoch
   -- this was fixed by adding a flag called 'self.state_initialized' that is set to True after the first call to self.generate_state() (which is guarded by an if statement: 'if self.stateful and not self.state_initialized:' )

   - moved self.epochs and self.epoch from src/network.py under the 'if training:' block of the Network constructor (it was previously outside of the 'if training:' block)
   -- this required refactoring the teacher forcing factor logic in src/rnn.py to be inside an 'if training:' block as well
   -- previously, the teacher forcing factor logic (which depends on both self.epochs and self.epoch) was not guarded by an 'if training:' block, which meant that a teacher forcing factor would be calculated even during inference (even thought it is not used during inference)
   -- this meant that self.epochs and self.epoch always had to be exposed (even during inference), which meant that the declaration of these attributes could not be moved under an 'if training:' block (as that would mean they are inaccessible during inference, causing an 'AttributeError' to be raised)
   -- anyway, i put both self.epochs/self.epoch and the teacher forcing factor logic under 'if training:' blocks, so this is no longer an issue (although the code for the latter looks a bit uglier now)

   - implemented lstm model following aforementioned architectural changes
   -- found ways to get both lstm and rnn to work in autoregressive mode. however, doing so requires a significant number of epochs and samples (10000+ epochs and 10000+ samples) 
   -- even still, the phase of the sine wave is not learned properly
   -- regardless, something works at least. this is a good start, but there is still work to be done to make both models more robust 
   -- another next step is to try to implement a text_lstm model and re-test the old text_rnn model with this new knowledge that more epochs + more data leads to better results (duh)

   - added /backup directory to store training logs/runs that show good results

transformer (06/21/2025)
   - refactored layer.py to 'functions.py' and swapped out the Layer class for a collection of pure functions
   
   - fixed dumb batchnorm issue where bn2 in convolutional.py and bn1 in dense.py would be instantiated WITHIN traverse() and forward() methods, respectively, causing the internal state of the batchnorm layers to be reset every time a forward pass was made
   -- this is obviously dumb as it causes the internal state/statistics of the batchnorm layers to be reset every time, defeating the purpose/benefit of batch normalization
   -- this was fixed by moving the instantiation of the batchnorm object to their respective __init__() methods


transformer (06/28/2025)
   - added override decorator to cnn backprop() method and copied generic version of backprop() method to network.py
   -- this way, there's less copied code and the backprop() method can be overridden in any model that needs to make specific implementation changes
   -- for example, mlp, rnn, lstm all use the generic backprop() functionality while cnn() has its own specific implementation
   -- thus, mlp, rnn, and lstm can simply inherit from Network version while cnn can override it

transformer (07/10/2025)
   - add tokenizer and positional encoding to transformer model
   - work on transformer training
   - add attention mask to encoder and decoder (in addition to causal mask already present in decoder)


